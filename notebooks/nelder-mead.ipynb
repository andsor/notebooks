{
 "metadata": {},
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The Nelder--Mead algorithm\n",
      "\n",
      "The Nelder--Mead algorithm \n",
      "<cite data-cite=\"Nelder1965Simplex\">([Nelder & Mead, 1965])</cite>\n",
      "attempts to minimize a goal function $f : \\mathbb{R}^n \\to \\mathbb{R}$ of an\n",
      "unconstrained optimization problem.\n",
      "As it only evaluates function values, but no derivatives, the Nelder--Mead\n",
      "algorithm is a *direct search method* \n",
      "<cite data-cite=\"Kolda2003Optimization\">([Kolda et al., 2003])</cite>.\n",
      "Although the method generally lacks rigorous convergence properties \n",
      "<cite data-cite=\"Lagarias1998Convergence\">([Lagarias et al., 1998])</cite> \n",
      "<cite data-cite=\"Price2002Convergent\">([Price et al., 2002])</cite>, in\n",
      "practice the first few iterations often yield satisfactory results <cite\n",
      "data-cite=\"Singer2009NelderMead\">([Singer & Mead, 2009])</cite>.\n",
      "Typically, each iteration evaluates the goal function only once or twice \n",
      "<cite data-cite=\"Singer2004Efficient\">([Singer & Singer, 2004])</cite>,\n",
      "which is why the Nelder--Mead algorithm is comparatively fast if goal function\n",
      "evaluation is the computational bottleneck \n",
      "<cite data-cite=\"Singer2009NelderMead\">([Singer & Mead, 2009])</cite>.\n",
      "\n",
      "# The algorithm\n",
      "Nelder & Mead <cite data-cite=\"Nelder1965Simplex\">([Nelder & Mead,\n",
      "1965])</cite> refined a simplex method by Spendley et al. \n",
      "<cite data-cite=\"Spendley1962Sequential\">([Spendley et al., 1962])</cite>.\n",
      "A simplex is the generalization of triangles in $\\mathbb{R}^2$ to $n$\n",
      "dimensions: in $\\mathbb{R}^n$, a simplex is the convex hull of $n+1$ vertices\n",
      "$x_0, \\ldots, x_n \\in \\mathbb{R}^n$.\n",
      "Starting with an initial simplex, the algorithm attempts to decrease the\n",
      "function values $f_i := f(x_i)$ at the vertices by a sequence of elementary\n",
      "transformations of the simplex along the local landscape.\n",
      "The algorithm *succeeds* when the simplex is sufficiently small (*domain\n",
      "convergence test*), and/or when the function values $f_i$ are sufficiently close\n",
      "(*function-value convergence test*).\n",
      "The algorithm *fails* when it did not succeed after a given number of\n",
      "iterations or function evaluations.\n",
      "See Singer & Nead <cite data-cite=\"Singer2009NelderMead\">([Singer & Mead,\n",
      "2009])</cite> and references therein for a complete description of the\n",
      "algorithm and the simplex transformations.\n",
      "\n",
      "# Uncertainties in parameter estimation\n",
      "\n",
      "For parameter estimation, Spendley et al. <cite\n",
      "data-cite=\"Spendley1962Sequential\">([Spendley et al., 1962])</cite> and Nelder\n",
      "& Mead <cite data-cite=\"Nelder1965Simplex\">([Nelder & Mead,\n",
      "1965])</cite> provide a method to estimate the uncertainties.\n",
      "Fitting a quadratic surface to the vertices and the midpoints of the edges of\n",
      "the final simplex yields an estimate for the variance--covariance matrix.\n",
      "The variance--covariance matrix is $\\mathbf{Q} \\mathbf{B}^{-1} \\mathbf{Q}^T$ as\n",
      "originally given by Nelder & Mead <cite data-cite=\"Nelder1965Simplex\">([Nelder\n",
      "& Mead, 1965])</cite>, despite the erratum on the original paper.\n",
      "The errors are the square roots of the diagonal terms \n",
      "<cite data-cite=\"Bevington2003Data\">([Bevington & Robinson, 2003])</cite>.\n",
      "\n",
      "# Implementation\n",
      "\n",
      "Scientific Python \n",
      "<cite data-cite=\"Jones2001SciPy\">([Jones et al., 2001])</cite> \n",
      "<cite data-cite=\"Oliphant2007Python\">([Oliphant, 2007])</cite>\n",
      "implements the Nelder--Mead method for the [scipy.optimize.minimize]\n",
      "function.\n",
      "Note that this implementation only returns the vertex with the lowest function\n",
      "value, but not the whole final simplex.\n",
      "\n",
      "[scipy.optimize.minimize]: http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
      "\n",
      "[Oliphant, 2007]: http://dx.doi.org/10.1109/mcse.2007.58\n",
      "\n",
      "[Jones et al., 2001]: http://www.scipy.org\n",
      "\n",
      "[Spendley et al., 1962]: http://dx.doi.org/10.1080/00401706.1962.10490033\n",
      "\n",
      "[Singer & Mead, 2009]: http://dx.doi.org/10.4249/scholarpedia.2928\n",
      "\n",
      "[Singer & Singer, 2004]: http://dx.doi.org/10.1002/anac.200410015\n",
      "\n",
      "[Price et al., 2002]: http://dx.doi.org/10.1023/a%3a1014849028575\n",
      "\n",
      "[Lagarias et al., 1998]: http://dx.doi.org/10.1137/s1052623496303470\n",
      "\n",
      "[Kolda et al., 2003]: http://dx.doi.org/10.1137/s003614450242889\n",
      "\n",
      "[Nelder & Mead, 1965]: http://dx.doi.org/10.1093/comjnl/7.4.308"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ******NOTICE***************\n",
      "# optimize.py module by Travis E. Oliphant\n",
      "#\n",
      "# You may copy and use this module as you see fit with no\n",
      "# guarantee implied provided you keep this notice in all copies.\n",
      "# *****END NOTICE************\n",
      "\n",
      "# numpy\n",
      "import numpy\n",
      "import scipy.optimize\n",
      "\n",
      "from numpy import (\n",
      "    asfarray\n",
      ")\n",
      "\n",
      "from scipy.optimize.optimize import (\n",
      "    _check_unknown_options, wrap_function, _status_message, OptimizeResult\n",
      ")\n",
      "\n",
      "\n",
      "def _neldermead_errors(\n",
      "    sim, fsim, func\n",
      "):\n",
      "    # fit quadratic coefficients\n",
      "    fun = func\n",
      "\n",
      "    n = len(sim) - 1\n",
      "\n",
      "    x = 0.5 * (sim[numpy.mgrid[0:6, 0:6]][1] + sim[numpy.mgrid[0:6, 0:6]][0])\n",
      "\n",
      "    for i in range(n + 1):\n",
      "        assert(numpy.array_equal(x[i,i], sim[i]))\n",
      "        for j in range(n + 1):\n",
      "            assert(numpy.array_equal(x[i,j], 0.5 * (sim[i] + sim[j])))\n",
      "\n",
      "    y = numpy.nan * numpy.ones(shape=(n + 1, n + 1))\n",
      "    for i in range(n + 1):\n",
      "        y[i, i] = fsim[i]\n",
      "        for j in range(i + 1, n + 1):\n",
      "            y[i, j] = y[j, i] = fun(x[i, j])\n",
      "\n",
      "    y0i = y[numpy.mgrid[0:6, 0:6]][0][1:,1:, 0]\n",
      "    for i in range(n):\n",
      "        for j in range(n):\n",
      "            assert y0i[i, j] == y[0, i + 1], (i, j)\n",
      "\n",
      "    y0j = y[numpy.mgrid[0:6, 0:6]][0][0, 1:, 1:]\n",
      "    for i in range(n):\n",
      "        for j in range(n):\n",
      "            assert y0j[i, j] == y[0, j + 1], (i, j)\n",
      "\n",
      "    b = 2 * (y[1:, 1:] + y[0, 0] - y0i - y0j)\n",
      "    for i in range(n):\n",
      "        assert abs(b[i, i] - 2 * (fsim[i + 1] + fsim[0] - 2 * y[0, i + 1])) < 1e-12\n",
      "        for j in range(n):\n",
      "            if i == j:\n",
      "                continue\n",
      "            assert abs(b[i, j] - 2 * (y[i + 1, j + 1] + fsim[0] - y[0, i + 1] -\n",
      "                y[0, j + 1])) < 1e-12\n",
      "\n",
      "    q = (sim - sim[0])[1:].T\n",
      "    for i in range(n):\n",
      "        assert numpy.array_equal(q[:, i], sim[i + 1] - sim[0])\n",
      "    \n",
      "    varco = numpy.dot(q, numpy.dot(numpy.linalg.inv(b), q.T))\n",
      "    return numpy.sqrt(numpy.diag(varco))\n",
      "\n",
      "def minimize_neldermead_witherrors(\n",
      "    fun, x0, args=(), callback=None,\n",
      "    xtol=1e-4, ftol=1e-4, maxiter=None, maxfev=None,\n",
      "    disp=False, return_all=False, with_errors=True,\n",
      "    **unknown_options):\n",
      "    \"\"\"\n",
      "    Minimization of scalar function of one or more variables using the\n",
      "    Nelder-Mead algorithm.\n",
      "\n",
      "    Options for the Nelder-Mead algorithm are:\n",
      "        disp : bool\n",
      "            Set to True to print convergence messages.\n",
      "        xtol : float\n",
      "            Relative error in solution `xopt` acceptable for convergence.\n",
      "        ftol : float\n",
      "            Relative error in ``fun(xopt)`` acceptable for convergence.\n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfev : int\n",
      "            Maximum number of function evaluations to make.\n",
      "\n",
      "    This function is called by the `minimize` function with\n",
      "    `method=minimize_neldermead_with_errors`. It is not supposed to be called directly.\n",
      "    \"\"\"\n",
      "    maxfun = maxfev\n",
      "    retall = return_all\n",
      "\n",
      "    fcalls, func = wrap_function(fun, args)\n",
      "    x0 = asfarray(x0).flatten()\n",
      "    N = len(x0)\n",
      "    rank = len(x0.shape)\n",
      "    if not -1 < rank < 2:\n",
      "        raise ValueError(\"Initial guess must be a scalar or rank-1 sequence.\")\n",
      "    if maxiter is None:\n",
      "        maxiter = N * 200\n",
      "    if maxfun is None:\n",
      "        maxfun = N * 200\n",
      "\n",
      "    rho = 1\n",
      "    chi = 2\n",
      "    psi = 0.5\n",
      "    sigma = 0.5\n",
      "    one2np1 = list(range(1, N + 1))\n",
      "\n",
      "    if rank == 0:\n",
      "        sim = numpy.zeros((N + 1,), dtype=x0.dtype)\n",
      "    else:\n",
      "        sim = numpy.zeros((N + 1, N), dtype=x0.dtype)\n",
      "    fsim = numpy.zeros((N + 1,), float)\n",
      "    sim[0] = x0\n",
      "    if retall:\n",
      "        allvecs = [sim[0]]\n",
      "    fsim[0] = func(x0)\n",
      "    nonzdelt = 0.05\n",
      "    zdelt = 0.00025\n",
      "    for k in range(0, N):\n",
      "        y = numpy.array(x0, copy=True)\n",
      "        if y[k] != 0:\n",
      "            y[k] = (1 + nonzdelt)*y[k]\n",
      "        else:\n",
      "            y[k] = zdelt\n",
      "\n",
      "        sim[k + 1] = y\n",
      "        f = func(y)\n",
      "        fsim[k + 1] = f\n",
      "\n",
      "    ind = numpy.argsort(fsim)\n",
      "    fsim = numpy.take(fsim, ind, 0)\n",
      "    # sort so sim[0,:] has the lowest function value\n",
      "    sim = numpy.take(sim, ind, 0)\n",
      "\n",
      "    iterations = 1\n",
      "\n",
      "    while (fcalls[0] < maxfun and iterations < maxiter):\n",
      "        if (numpy.max(numpy.ravel(numpy.abs(sim[1:] - sim[0]))) <= xtol and\n",
      "                numpy.max(numpy.abs(fsim[0] - fsim[1:])) <= ftol):\n",
      "            break\n",
      "\n",
      "        xbar = numpy.add.reduce(sim[:-1], 0) / N\n",
      "        xr = (1 + rho) * xbar - rho * sim[-1]\n",
      "        fxr = func(xr)\n",
      "        doshrink = 0\n",
      "\n",
      "        if fxr < fsim[0]:\n",
      "            xe = (1 + rho * chi) * xbar - rho * chi * sim[-1]\n",
      "            fxe = func(xe)\n",
      "\n",
      "            if fxe < fxr:\n",
      "                sim[-1] = xe\n",
      "                fsim[-1] = fxe\n",
      "            else:\n",
      "                sim[-1] = xr\n",
      "                fsim[-1] = fxr\n",
      "        else:  # fsim[0] <= fxr\n",
      "            if fxr < fsim[-2]:\n",
      "                sim[-1] = xr\n",
      "                fsim[-1] = fxr\n",
      "            else:  # fxr >= fsim[-2]\n",
      "                # Perform contraction\n",
      "                if fxr < fsim[-1]:\n",
      "                    xc = (1 + psi * rho) * xbar - psi * rho * sim[-1]\n",
      "                    fxc = func(xc)\n",
      "\n",
      "                    if fxc <= fxr:\n",
      "                        sim[-1] = xc\n",
      "                        fsim[-1] = fxc\n",
      "                    else:\n",
      "                        doshrink = 1\n",
      "                else:\n",
      "                    # Perform an inside contraction\n",
      "                    xcc = (1 - psi) * xbar + psi * sim[-1]\n",
      "                    fxcc = func(xcc)\n",
      "\n",
      "                    if fxcc < fsim[-1]:\n",
      "                        sim[-1] = xcc\n",
      "                        fsim[-1] = fxcc\n",
      "                    else:\n",
      "                        doshrink = 1\n",
      "\n",
      "                if doshrink:\n",
      "                    for j in one2np1:\n",
      "                        sim[j] = sim[0] + sigma * (sim[j] - sim[0])\n",
      "                        fsim[j] = func(sim[j])\n",
      "\n",
      "        ind = numpy.argsort(fsim)\n",
      "        sim = numpy.take(sim, ind, 0)\n",
      "        fsim = numpy.take(fsim, ind, 0)\n",
      "        if callback is not None:\n",
      "            callback(sim[0])\n",
      "        iterations += 1\n",
      "        if retall:\n",
      "            allvecs.append(sim[0])\n",
      "\n",
      "    x = sim[0]\n",
      "    fval = numpy.min(fsim)\n",
      "    warnflag = 0\n",
      "    errors = None\n",
      "\n",
      "    if fcalls[0] >= maxfun:\n",
      "        warnflag = 1\n",
      "        msg = _status_message['maxfev']\n",
      "        if disp:\n",
      "            print('Warning: ' + msg)\n",
      "    elif iterations >= maxiter:\n",
      "        warnflag = 2\n",
      "        msg = _status_message['maxiter']\n",
      "        if disp:\n",
      "            print('Warning: ' + msg)\n",
      "    else:\n",
      "        msg = _status_message['success']\n",
      "        errors = _neldermead_errors(sim, fsim, func)       \n",
      "        if disp:\n",
      "            print(msg)\n",
      "            print(\"         Current function value: %f\" % fval)\n",
      "            print(\"         Iterations: %d\" % iterations)\n",
      "            print(\"         Function evaluations: %d\" % fcalls[0])\n",
      "\n",
      "    result = OptimizeResult(fun=fval, nit=iterations, nfev=fcalls[0],\n",
      "                            status=warnflag, success=(warnflag == 0),\n",
      "\t\t\t    message=msg, x=x, errors=errors, sim=sim,\n",
      "                            fsim=fsim)\n",
      "\n",
      "    if retall:\n",
      "        result['allvecs'] = allvecs\n",
      "    return result\n",
      "\n",
      "x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "res = scipy.optimize.minimize(scipy.optimize.rosen, x0, method='Nelder-Mead')\n",
      "print(res)\n",
      "\n",
      "res_witherrors = scipy.optimize.minimize(\n",
      "    scipy.optimize.rosen,\n",
      "    x0,\n",
      "    method=minimize_neldermead_witherrors\n",
      ")\n",
      "print(res_witherrors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "       x: array([ 0.99910115,  0.99820923,  0.99646346,  0.99297555,  0.98600385])\n",
        "     nit: 141\n",
        " success: True\n",
        "     fun: 6.6174817088845322e-05\n",
        " message: 'Optimization terminated successfully.'\n",
        "    nfev: 243\n",
        "  status: 0\n",
        " success: True\n",
        "    fsim: array([  6.61748171e-05,   6.64266969e-05,   6.66640269e-05,\n",
        "         6.69424827e-05,   6.70671859e-05,   6.70870519e-05])\n",
        "     sim: array([[ 0.99910115,  0.99820923,  0.99646346,  0.99297555,  0.98600385],\n",
        "       [ 0.99909442,  0.99820319,  0.99645812,  0.99302291,  0.98608273],\n",
        "       [ 0.99908478,  0.99820409,  0.9964653 ,  0.99296511,  0.9859391 ],\n",
        "       [ 0.99909641,  0.99824295,  0.99644354,  0.9929541 ,  0.98596017],\n",
        "       [ 0.99907389,  0.998211  ,  0.99645267,  0.99294537,  0.98597485],\n",
        "       [ 0.99907893,  0.99819173,  0.99644522,  0.99298154,  0.986004  ]])\n",
        "     fun: 6.6174817088845322e-05\n",
        "     nit: 141\n",
        "       x: array([ 0.99910115,  0.99820923,  0.99646346,  0.99297555,  0.98600385])\n",
        "  errors: array([ 0.12236908,  0.22373152,  0.43670037,  0.86737782,  1.72549539])\n",
        " message: 'Optimization terminated successfully.'\n",
        "  status: 0\n",
        "    nfev: 258"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> *[This work](http://notebooks.asorge.de) is licensed under a [Creative\n",
      "Commons Attribution 4.0 International\n",
      "License](http://creativecommons.org/licenses/by/4.0/).*\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}